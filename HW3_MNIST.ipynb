{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_MNIST",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinWalnut/ABR_basic/blob/master/HW3_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BJBR34aJ0DHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "be0ef715-d829-41c0-9901-576fc7e0f8fa"
      },
      "cell_type": "code",
      "source": [
        "#HW3 - MNIST\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#데이터 불러오기\n",
        "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
        "\n",
        "print(\"훈련 이미지 :\",  mnist.train.images.shape)\n",
        "print(\"훈련 라벨:\",  mnist.train.labels.shape)\n",
        "print(\"테스트 이미지 : \", mnist.test.images.shape)\n",
        "print(\"테스트 라벨 : \", mnist.test.labels.shape)\n",
        "print(\"검증 이미지 : \", mnist.validation.images.shape)\n",
        "print(\"검증 라벨 : \", mnist.validation.labels.shape)\n",
        "\n",
        "# placeholder\n",
        "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
        "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
        "\n",
        "# parameter\n",
        "epoch = 25\n",
        "mini_batch_size = 500\n",
        "total_batch_size = int(mnist.train.num_examples / mini_batch_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n",
            "훈련 이미지 : (55000, 784)\n",
            "훈련 라벨: (55000, 10)\n",
            "테스트 이미지 :  (10000, 784)\n",
            "테스트 라벨 :  (10000, 10)\n",
            "검증 이미지 :  (5000, 784)\n",
            "검증 라벨 :  (5000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iP64V9G1C3uZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "78cbcae5-5df6-4075-c78b-38b559c46c72"
      },
      "cell_type": "code",
      "source": [
        "#모델 생성 및 학습\n",
        "\n",
        "#레이어 구조\n",
        "h_layer1 = tf.layers.dense(X, 300, activation = tf.nn.relu)\n",
        "h_layer2 = tf.layers.dense(h_layer1, 300, activation = tf.nn.relu)\n",
        "output_layer = tf.layers.dense(h_layer2, 10)\n",
        "\n",
        "#loss training model\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels = Y, logits = output_layer)\n",
        "train_optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
        "\n",
        "#accuracy 측정\n",
        "correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1)) # one-hot 벡터에서 1을 가지는 인덱스가 서로 같은지 비교하여 같으면 true 반환\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "#세션 시작\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for epo in range(epoch):\n",
        "    avg_loss = 0\n",
        "    \n",
        "    for bat in range(total_batch_size):\n",
        "      #batch 할당\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(mini_batch_size)\n",
        "      feed_dict={X: batch_xs, Y: batch_ys}\n",
        "      \n",
        "      #세션 실행 -> hist_loss에 현재 loss값 저장\n",
        "      hist_loss, _ = sess.run([loss, train_optimizer], feed_dict=feed_dict)\n",
        "      avg_loss += hist_loss / total_batch_size\n",
        "    print(\"Epoch :\", '%02d' % (epo+1), \"loss = \", \"{:.9f}\".format(avg_loss))\n",
        "  \n",
        "  print(\"학습 종료\")\n",
        "  print(\"Accuracy : {:.3f}%\".format(100.*sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 01 loss =  0.410304549\n",
            "Epoch : 02 loss =  0.146491909\n",
            "Epoch : 03 loss =  0.098489930\n",
            "Epoch : 04 loss =  0.071706864\n",
            "Epoch : 05 loss =  0.053760971\n",
            "Epoch : 06 loss =  0.039651807\n",
            "Epoch : 07 loss =  0.030889571\n",
            "Epoch : 08 loss =  0.023197308\n",
            "Epoch : 09 loss =  0.017316716\n",
            "Epoch : 10 loss =  0.014401294\n",
            "Epoch : 11 loss =  0.012559640\n",
            "Epoch : 12 loss =  0.010798892\n",
            "Epoch : 13 loss =  0.007048921\n",
            "Epoch : 14 loss =  0.005134160\n",
            "Epoch : 15 loss =  0.003067976\n",
            "Epoch : 16 loss =  0.002241491\n",
            "Epoch : 17 loss =  0.001543585\n",
            "Epoch : 18 loss =  0.001161252\n",
            "Epoch : 19 loss =  0.000845131\n",
            "Epoch : 20 loss =  0.000703636\n",
            "Epoch : 21 loss =  0.000619740\n",
            "Epoch : 22 loss =  0.000584165\n",
            "Epoch : 23 loss =  0.000457953\n",
            "Epoch : 24 loss =  0.000397026\n",
            "Epoch : 25 loss =  0.000350336\n",
            "학습 종료\n",
            "Accuracy : 98.230%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}