{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABR_HW4_MNIST_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinWalnut/ABR_basic/blob/master/ABR_HW4_MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "upBEKV8Ws1d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "29a9d89e-9add-455c-8b87-b49ee276dd52"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#데이터 불러오기\n",
        "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
        "\n",
        "# parameter\n",
        "epoch = 10\n",
        "mini_batch_size = 500\n",
        "total_batch_size = int(mnist.train.num_examples / mini_batch_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgDWDNBivY9r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 모델 생성\n",
        "\n",
        "## place holder 생성\n",
        "X = tf.placeholder(shape = [None, 28, 28, 1], dtype = tf.float32) # MNIST 이미지[28,28,1] 데이터 입력\n",
        "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32) # MNIST 라벨 데이터 입력\n",
        "\n",
        "## 모델 레이어 구조 생성\n",
        "# output 이미지의 사이즈 계산 : NxN image * FxF filter * SxS strides => (N-F)/S + 1 (계산 값이 정수가 되도록 설정)\n",
        "# conv(@20) - max_pool- conv(@32) - average_pool - mlp(분류)\n",
        "conv_1 = tf.layers.conv2d(X, filters = 20, kernel_size = [3, 3], strides = [1, 1], padding = \"same\", activation = tf.nn.relu) #batch x 28 x 28 @ 20\n",
        "pool_1 = tf.layers.max_pooling2d(conv_1, pool_size = [2, 2], strides = [2, 2], padding = \"valid\") #batch x 14 x 14 @ 20\n",
        "\n",
        "conv_2 = tf.layers.conv2d(pool_1, filters = 32, kernel_size = [3, 3], strides = [1, 1], padding = \"valid\", activation = tf.nn.relu) #batch x 12 x 12 @ 32\n",
        "pool_2 = tf.layers.average_pooling2d(conv_2, pool_size = [2, 2], strides = [2, 2], padding= \"valid\") #batch x 6 x 6 @ 32\n",
        "\n",
        "## 분류를 위한 MLP 출력단\n",
        "flat = tf.layers.flatten(inputs = pool_2)\n",
        "h_layer1 = tf.layers.dense(flat, 512, activation = tf.nn.relu)\n",
        "h_layer2 = tf.layers.dense(h_layer1, 128, activation = tf.nn.relu)\n",
        "output_layer = tf.layers.dense(h_layer2, 10)\n",
        "\n",
        "## loss training model\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels = Y, logits = output_layer)\n",
        "train_optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
        "\n",
        "## test를 위한 정확도 측정\n",
        "predict = tf.nn.softmax(output_layer) # 모델을 통해 출력된 데이터를 softmax에 통과시켜 모델이 예측한 값을 찾는다 \n",
        "correct = tf.cast(tf.equal(tf.argmax(Y, 1), tf.argmax(predict, 1)), tf.float32) # 라벨과 예측값이 맞다면 equl()은 True를 반환, 이를 cast()가 1로 바꾸어준다. (false는 0으로 바꾼다)\n",
        "accuracy = tf.reduce_mean(correct) # correct의 평균을 구한다. 0 ~ 1사이의 값. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZ-5Cb10AHak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2305
        },
        "outputId": "359327be-a25f-4e5f-cded-600e9e80ad3f"
      },
      "cell_type": "code",
      "source": [
        "#세션 시작\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for epo in range(epoch):\n",
        "    avg_train_loss, avg_valid_loss = 0, 0\n",
        "    \n",
        "    for bat in range(total_batch_size):\n",
        "      #batch 할당\n",
        "      train_xdata, train_labels = mnist.train.next_batch(mini_batch_size)\n",
        "      train_xdata = np.reshape(train_xdata, [-1, 28, 28, 1])\n",
        "      valid_xdata, valid_labels = mnist.validation.next_batch(mini_batch_size)\n",
        "      valid_xdata = np.reshape(valid_xdata, [-1, 28, 28, 1])\n",
        "      \n",
        "      #세션 실행\n",
        "      train_loss, _ = sess.run([loss, train_optimizer], feed_dict={X: train_xdata, Y: train_labels})\n",
        "      valid_loss = sess.run(loss, feed_dict= {X: valid_xdata, Y: valid_labels})\n",
        "      avg_train_loss += train_loss / total_batch_size\n",
        "      avg_valid_loss += valid_loss / total_batch_size\n",
        "      \n",
        "      acc_train = sess.run(accuracy, feed_dict={X: train_xdata, Y: train_labels})\n",
        "      acc_valid = sess.run(accuracy, feed_dict={X: valid_xdata, Y: valid_labels})\n",
        "      if epo==0 or (bat+1)%(total_batch_size/2)==0:\n",
        "        print(\"Epoch :\", '%02d' % (epo+1), \"Batch :\",'%03d'%(bat+1), \", avg_train_loss = \", \"{:.9f}\".format(avg_train_loss), \"(\", \"{:.3f}%\".format(100*acc_train), \")\",\n",
        "          \", avg_valid_loss = \", \"{:.9f}\".format(avg_valid_loss), \"(\", \"{:.3f}%\".format(100*acc_valid), \")\")\n",
        "  \n",
        "  print(\"학습 종료\")\n",
        "  \n",
        "  test_xdata, test_labels = mnist.test.next_batch(mini_batch_size)\n",
        "  test_xdata = np.reshape(test_xdata, [-1, 28, 28, 1])\n",
        "  print(\"Accuracy : {:.3f}%\".format(100.*sess.run(accuracy, feed_dict={X: test_xdata, Y: test_labels})))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 01 Batch : 001 , avg_train_loss =  0.020986585 ( 40.800% ) , avg_valid_loss =  0.020420445 ( 36.200% )\n",
            "Epoch : 01 Batch : 002 , avg_train_loss =  0.041403148 ( 58.400% ) , avg_valid_loss =  0.040231499 ( 52.800% )\n",
            "Epoch : 01 Batch : 003 , avg_train_loss =  0.061251207 ( 57.400% ) , avg_valid_loss =  0.059411814 ( 52.000% )\n",
            "Epoch : 01 Batch : 004 , avg_train_loss =  0.080420438 ( 54.600% ) , avg_valid_loss =  0.077744976 ( 55.600% )\n",
            "Epoch : 01 Batch : 005 , avg_train_loss =  0.098858448 ( 59.800% ) , avg_valid_loss =  0.095124383 ( 63.800% )\n",
            "Epoch : 01 Batch : 006 , avg_train_loss =  0.116519209 ( 66.600% ) , avg_valid_loss =  0.111427012 ( 67.600% )\n",
            "Epoch : 01 Batch : 007 , avg_train_loss =  0.132791561 ( 72.800% ) , avg_valid_loss =  0.126587278 ( 65.800% )\n",
            "Epoch : 01 Batch : 008 , avg_train_loss =  0.148048479 ( 71.800% ) , avg_valid_loss =  0.140607062 ( 69.000% )\n",
            "Epoch : 01 Batch : 009 , avg_train_loss =  0.161887730 ( 71.000% ) , avg_valid_loss =  0.153150717 ( 68.200% )\n",
            "Epoch : 01 Batch : 010 , avg_train_loss =  0.174865534 ( 70.400% ) , avg_valid_loss =  0.164294113 ( 71.800% )\n",
            "Epoch : 01 Batch : 011 , avg_train_loss =  0.186127136 ( 75.800% ) , avg_valid_loss =  0.174214953 ( 74.400% )\n",
            "Epoch : 01 Batch : 012 , avg_train_loss =  0.196492061 ( 74.000% ) , avg_valid_loss =  0.183012901 ( 76.200% )\n",
            "Epoch : 01 Batch : 013 , avg_train_loss =  0.205304823 ( 76.400% ) , avg_valid_loss =  0.190966257 ( 77.400% )\n",
            "Epoch : 01 Batch : 014 , avg_train_loss =  0.213474549 ( 76.600% ) , avg_valid_loss =  0.198283415 ( 76.600% )\n",
            "Epoch : 01 Batch : 015 , avg_train_loss =  0.221003820 ( 77.200% ) , avg_valid_loss =  0.204754428 ( 80.200% )\n",
            "Epoch : 01 Batch : 016 , avg_train_loss =  0.228040343 ( 79.200% ) , avg_valid_loss =  0.210570620 ( 81.800% )\n",
            "Epoch : 01 Batch : 017 , avg_train_loss =  0.234023574 ( 81.600% ) , avg_valid_loss =  0.216025783 ( 81.400% )\n",
            "Epoch : 01 Batch : 018 , avg_train_loss =  0.239866863 ( 81.200% ) , avg_valid_loss =  0.221222525 ( 83.400% )\n",
            "Epoch : 01 Batch : 019 , avg_train_loss =  0.245247597 ( 83.200% ) , avg_valid_loss =  0.226521062 ( 81.800% )\n",
            "Epoch : 01 Batch : 020 , avg_train_loss =  0.249985117 ( 84.800% ) , avg_valid_loss =  0.231182660 ( 84.200% )\n",
            "Epoch : 01 Batch : 021 , avg_train_loss =  0.254666370 ( 84.800% ) , avg_valid_loss =  0.236300967 ( 81.600% )\n",
            "Epoch : 01 Batch : 022 , avg_train_loss =  0.259241977 ( 86.800% ) , avg_valid_loss =  0.240875309 ( 83.800% )\n",
            "Epoch : 01 Batch : 023 , avg_train_loss =  0.264329420 ( 86.200% ) , avg_valid_loss =  0.245331658 ( 86.400% )\n",
            "Epoch : 01 Batch : 024 , avg_train_loss =  0.269808851 ( 84.200% ) , avg_valid_loss =  0.249812464 ( 85.000% )\n",
            "Epoch : 01 Batch : 025 , avg_train_loss =  0.274035849 ( 87.200% ) , avg_valid_loss =  0.254303219 ( 86.000% )\n",
            "Epoch : 01 Batch : 026 , avg_train_loss =  0.278350178 ( 88.200% ) , avg_valid_loss =  0.258531263 ( 84.800% )\n",
            "Epoch : 01 Batch : 027 , avg_train_loss =  0.282221769 ( 86.800% ) , avg_valid_loss =  0.261961345 ( 86.600% )\n",
            "Epoch : 01 Batch : 028 , avg_train_loss =  0.286993139 ( 84.600% ) , avg_valid_loss =  0.265738100 ( 88.000% )\n",
            "Epoch : 01 Batch : 029 , avg_train_loss =  0.291653911 ( 85.000% ) , avg_valid_loss =  0.269764176 ( 86.600% )\n",
            "Epoch : 01 Batch : 030 , avg_train_loss =  0.295213840 ( 88.000% ) , avg_valid_loss =  0.274094723 ( 87.200% )\n",
            "Epoch : 01 Batch : 031 , avg_train_loss =  0.299206346 ( 86.800% ) , avg_valid_loss =  0.277890801 ( 86.600% )\n",
            "Epoch : 01 Batch : 032 , avg_train_loss =  0.303398892 ( 86.800% ) , avg_valid_loss =  0.281582928 ( 89.000% )\n",
            "Epoch : 01 Batch : 033 , avg_train_loss =  0.308309888 ( 88.200% ) , avg_valid_loss =  0.285020306 ( 88.800% )\n",
            "Epoch : 01 Batch : 034 , avg_train_loss =  0.311777629 ( 89.400% ) , avg_valid_loss =  0.288606501 ( 90.600% )\n",
            "Epoch : 01 Batch : 035 , avg_train_loss =  0.315802015 ( 88.400% ) , avg_valid_loss =  0.292068018 ( 88.600% )\n",
            "Epoch : 01 Batch : 036 , avg_train_loss =  0.319413923 ( 88.400% ) , avg_valid_loss =  0.295346716 ( 89.600% )\n",
            "Epoch : 01 Batch : 037 , avg_train_loss =  0.323567094 ( 87.400% ) , avg_valid_loss =  0.298282523 ( 90.600% )\n",
            "Epoch : 01 Batch : 038 , avg_train_loss =  0.327010411 ( 91.200% ) , avg_valid_loss =  0.301128728 ( 91.600% )\n",
            "Epoch : 01 Batch : 039 , avg_train_loss =  0.329934012 ( 91.400% ) , avg_valid_loss =  0.304577839 ( 89.800% )\n",
            "Epoch : 01 Batch : 040 , avg_train_loss =  0.333349687 ( 88.400% ) , avg_valid_loss =  0.308086015 ( 89.000% )\n",
            "Epoch : 01 Batch : 041 , avg_train_loss =  0.336127953 ( 90.600% ) , avg_valid_loss =  0.310899413 ( 89.200% )\n",
            "Epoch : 01 Batch : 042 , avg_train_loss =  0.339788468 ( 89.400% ) , avg_valid_loss =  0.313736754 ( 90.000% )\n",
            "Epoch : 01 Batch : 043 , avg_train_loss =  0.342755455 ( 90.000% ) , avg_valid_loss =  0.316330354 ( 92.600% )\n",
            "Epoch : 01 Batch : 044 , avg_train_loss =  0.346202539 ( 88.600% ) , avg_valid_loss =  0.319194217 ( 90.600% )\n",
            "Epoch : 01 Batch : 045 , avg_train_loss =  0.349015299 ( 89.600% ) , avg_valid_loss =  0.321662515 ( 91.600% )\n",
            "Epoch : 01 Batch : 046 , avg_train_loss =  0.352196228 ( 89.600% ) , avg_valid_loss =  0.325368173 ( 89.200% )\n",
            "Epoch : 01 Batch : 047 , avg_train_loss =  0.354963658 ( 91.000% ) , avg_valid_loss =  0.328448042 ( 90.600% )\n",
            "Epoch : 01 Batch : 048 , avg_train_loss =  0.357666448 ( 90.400% ) , avg_valid_loss =  0.331125923 ( 91.400% )\n",
            "Epoch : 01 Batch : 049 , avg_train_loss =  0.360260313 ( 93.000% ) , avg_valid_loss =  0.333992298 ( 90.400% )\n",
            "Epoch : 01 Batch : 050 , avg_train_loss =  0.363332165 ( 89.800% ) , avg_valid_loss =  0.336065865 ( 93.400% )\n",
            "Epoch : 01 Batch : 051 , avg_train_loss =  0.365630629 ( 94.200% ) , avg_valid_loss =  0.338249620 ( 92.600% )\n",
            "Epoch : 01 Batch : 052 , avg_train_loss =  0.368825774 ( 89.800% ) , avg_valid_loss =  0.340779578 ( 91.800% )\n",
            "Epoch : 01 Batch : 053 , avg_train_loss =  0.371384248 ( 91.600% ) , avg_valid_loss =  0.343831138 ( 91.200% )\n",
            "Epoch : 01 Batch : 054 , avg_train_loss =  0.374378796 ( 92.400% ) , avg_valid_loss =  0.346501895 ( 92.600% )\n",
            "Epoch : 01 Batch : 055 , avg_train_loss =  0.376856509 ( 93.200% ) , avg_valid_loss =  0.349222981 ( 91.600% )\n",
            "Epoch : 01 Batch : 056 , avg_train_loss =  0.380018475 ( 88.200% ) , avg_valid_loss =  0.351556292 ( 92.600% )\n",
            "Epoch : 01 Batch : 057 , avg_train_loss =  0.382472779 ( 93.200% ) , avg_valid_loss =  0.353759170 ( 93.600% )\n",
            "Epoch : 01 Batch : 058 , avg_train_loss =  0.384852287 ( 93.000% ) , avg_valid_loss =  0.356392607 ( 90.400% )\n",
            "Epoch : 01 Batch : 059 , avg_train_loss =  0.387285401 ( 92.400% ) , avg_valid_loss =  0.358387973 ( 94.800% )\n",
            "Epoch : 01 Batch : 060 , avg_train_loss =  0.389978655 ( 92.400% ) , avg_valid_loss =  0.360587419 ( 91.600% )\n",
            "Epoch : 01 Batch : 061 , avg_train_loss =  0.392663671 ( 91.400% ) , avg_valid_loss =  0.363220394 ( 91.200% )\n",
            "Epoch : 01 Batch : 062 , avg_train_loss =  0.394988970 ( 92.600% ) , avg_valid_loss =  0.365378582 ( 92.800% )\n",
            "Epoch : 01 Batch : 063 , avg_train_loss =  0.397112620 ( 93.400% ) , avg_valid_loss =  0.368094749 ( 91.200% )\n",
            "Epoch : 01 Batch : 064 , avg_train_loss =  0.399441279 ( 92.400% ) , avg_valid_loss =  0.370485187 ( 92.800% )\n",
            "Epoch : 01 Batch : 065 , avg_train_loss =  0.402143698 ( 91.200% ) , avg_valid_loss =  0.372549019 ( 93.600% )\n",
            "Epoch : 01 Batch : 066 , avg_train_loss =  0.404273975 ( 93.600% ) , avg_valid_loss =  0.374697531 ( 93.000% )\n",
            "Epoch : 01 Batch : 067 , avg_train_loss =  0.406880115 ( 91.200% ) , avg_valid_loss =  0.376414060 ( 94.600% )\n",
            "Epoch : 01 Batch : 068 , avg_train_loss =  0.409284598 ( 92.000% ) , avg_valid_loss =  0.378015093 ( 95.800% )\n",
            "Epoch : 01 Batch : 069 , avg_train_loss =  0.411164176 ( 93.200% ) , avg_valid_loss =  0.380402507 ( 93.000% )\n",
            "Epoch : 01 Batch : 070 , avg_train_loss =  0.413601297 ( 91.600% ) , avg_valid_loss =  0.382785230 ( 91.200% )\n",
            "Epoch : 01 Batch : 071 , avg_train_loss =  0.416220576 ( 93.400% ) , avg_valid_loss =  0.385529703 ( 91.800% )\n",
            "Epoch : 01 Batch : 072 , avg_train_loss =  0.418320090 ( 93.000% ) , avg_valid_loss =  0.387582350 ( 94.000% )\n",
            "Epoch : 01 Batch : 073 , avg_train_loss =  0.421209058 ( 90.400% ) , avg_valid_loss =  0.389177735 ( 94.600% )\n",
            "Epoch : 01 Batch : 074 , avg_train_loss =  0.423141046 ( 94.600% ) , avg_valid_loss =  0.390990857 ( 94.400% )\n",
            "Epoch : 01 Batch : 075 , avg_train_loss =  0.425481175 ( 92.400% ) , avg_valid_loss =  0.392906607 ( 95.200% )\n",
            "Epoch : 01 Batch : 076 , avg_train_loss =  0.427149890 ( 95.000% ) , avg_valid_loss =  0.395334363 ( 92.200% )\n",
            "Epoch : 01 Batch : 077 , avg_train_loss =  0.429296079 ( 93.800% ) , avg_valid_loss =  0.397152074 ( 93.600% )\n",
            "Epoch : 01 Batch : 078 , avg_train_loss =  0.431965876 ( 91.400% ) , avg_valid_loss =  0.399341555 ( 92.200% )\n",
            "Epoch : 01 Batch : 079 , avg_train_loss =  0.434676130 ( 91.000% ) , avg_valid_loss =  0.401230034 ( 94.000% )\n",
            "Epoch : 01 Batch : 080 , avg_train_loss =  0.436937241 ( 93.800% ) , avg_valid_loss =  0.403038171 ( 94.400% )\n",
            "Epoch : 01 Batch : 081 , avg_train_loss =  0.439791653 ( 91.200% ) , avg_valid_loss =  0.404747421 ( 94.800% )\n",
            "Epoch : 01 Batch : 082 , avg_train_loss =  0.441428453 ( 93.800% ) , avg_valid_loss =  0.406658334 ( 91.800% )\n",
            "Epoch : 01 Batch : 083 , avg_train_loss =  0.444043557 ( 90.800% ) , avg_valid_loss =  0.409237619 ( 92.800% )\n",
            "Epoch : 01 Batch : 084 , avg_train_loss =  0.445886018 ( 94.800% ) , avg_valid_loss =  0.411097601 ( 94.200% )\n",
            "Epoch : 01 Batch : 085 , avg_train_loss =  0.448251110 ( 93.800% ) , avg_valid_loss =  0.412651595 ( 93.800% )\n",
            "Epoch : 01 Batch : 086 , avg_train_loss =  0.450208049 ( 93.600% ) , avg_valid_loss =  0.414372938 ( 94.200% )\n",
            "Epoch : 01 Batch : 087 , avg_train_loss =  0.452365114 ( 92.800% ) , avg_valid_loss =  0.416256582 ( 94.600% )\n",
            "Epoch : 01 Batch : 088 , avg_train_loss =  0.454070302 ( 94.200% ) , avg_valid_loss =  0.418008291 ( 95.000% )\n",
            "Epoch : 01 Batch : 089 , avg_train_loss =  0.455706773 ( 94.800% ) , avg_valid_loss =  0.420119115 ( 93.800% )\n",
            "Epoch : 01 Batch : 090 , avg_train_loss =  0.457253075 ( 95.800% ) , avg_valid_loss =  0.421951361 ( 93.800% )\n",
            "Epoch : 01 Batch : 091 , avg_train_loss =  0.459355977 ( 93.600% ) , avg_valid_loss =  0.423680585 ( 94.400% )\n",
            "Epoch : 01 Batch : 092 , avg_train_loss =  0.460887146 ( 95.600% ) , avg_valid_loss =  0.425731415 ( 93.200% )\n",
            "Epoch : 01 Batch : 093 , avg_train_loss =  0.462099346 ( 97.000% ) , avg_valid_loss =  0.427350173 ( 95.400% )\n",
            "Epoch : 01 Batch : 094 , avg_train_loss =  0.464064656 ( 94.000% ) , avg_valid_loss =  0.428800395 ( 95.800% )\n",
            "Epoch : 01 Batch : 095 , avg_train_loss =  0.465948083 ( 93.800% ) , avg_valid_loss =  0.430156210 ( 95.800% )\n",
            "Epoch : 01 Batch : 096 , avg_train_loss =  0.467772734 ( 94.600% ) , avg_valid_loss =  0.431838570 ( 93.800% )\n",
            "Epoch : 01 Batch : 097 , avg_train_loss =  0.469849465 ( 92.800% ) , avg_valid_loss =  0.433436601 ( 95.200% )\n",
            "Epoch : 01 Batch : 098 , avg_train_loss =  0.471037500 ( 96.000% ) , avg_valid_loss =  0.435308185 ( 94.000% )\n",
            "Epoch : 01 Batch : 099 , avg_train_loss =  0.472497309 ( 94.800% ) , avg_valid_loss =  0.436425801 ( 96.200% )\n",
            "Epoch : 01 Batch : 100 , avg_train_loss =  0.473885912 ( 94.800% ) , avg_valid_loss =  0.438420947 ( 93.600% )\n",
            "Epoch : 01 Batch : 101 , avg_train_loss =  0.475733117 ( 94.800% ) , avg_valid_loss =  0.439636310 ( 95.800% )\n",
            "Epoch : 01 Batch : 102 , avg_train_loss =  0.477049352 ( 95.200% ) , avg_valid_loss =  0.441440609 ( 93.600% )\n",
            "Epoch : 01 Batch : 103 , avg_train_loss =  0.478512778 ( 96.000% ) , avg_valid_loss =  0.442587950 ( 96.600% )\n",
            "Epoch : 01 Batch : 104 , avg_train_loss =  0.480392393 ( 93.600% ) , avg_valid_loss =  0.443813371 ( 95.800% )\n",
            "Epoch : 01 Batch : 105 , avg_train_loss =  0.481482287 ( 97.400% ) , avg_valid_loss =  0.445194592 ( 95.800% )\n",
            "Epoch : 01 Batch : 106 , avg_train_loss =  0.482779208 ( 95.400% ) , avg_valid_loss =  0.447264979 ( 94.600% )\n",
            "Epoch : 01 Batch : 107 , avg_train_loss =  0.484180415 ( 96.000% ) , avg_valid_loss =  0.448648387 ( 95.400% )\n",
            "Epoch : 01 Batch : 108 , avg_train_loss =  0.485502539 ( 96.200% ) , avg_valid_loss =  0.450361037 ( 95.200% )\n",
            "Epoch : 01 Batch : 109 , avg_train_loss =  0.487272263 ( 95.000% ) , avg_valid_loss =  0.451540304 ( 96.400% )\n",
            "Epoch : 01 Batch : 110 , avg_train_loss =  0.488821227 ( 96.600% ) , avg_valid_loss =  0.452657939 ( 96.000% )\n",
            "Epoch : 02 Batch : 055 , avg_train_loss =  0.062812659 ( 98.400% ) , avg_valid_loss =  0.055793930 ( 97.200% )\n",
            "Epoch : 02 Batch : 110 , avg_train_loss =  0.109370681 ( 98.000% ) , avg_valid_loss =  0.098783714 ( 97.800% )\n",
            "Epoch : 03 Batch : 055 , avg_train_loss =  0.032938517 ( 99.200% ) , avg_valid_loss =  0.031868440 ( 97.800% )\n",
            "Epoch : 03 Batch : 110 , avg_train_loss =  0.065358698 ( 98.400% ) , avg_valid_loss =  0.061325449 ( 99.400% )\n",
            "Epoch : 04 Batch : 055 , avg_train_loss =  0.026286639 ( 98.600% ) , avg_valid_loss =  0.026850927 ( 98.800% )\n",
            "Epoch : 04 Batch : 110 , avg_train_loss =  0.051041026 ( 99.000% ) , avg_valid_loss =  0.051121871 ( 99.000% )\n",
            "Epoch : 05 Batch : 055 , avg_train_loss =  0.019661862 ( 98.600% ) , avg_valid_loss =  0.023512580 ( 98.600% )\n",
            "Epoch : 05 Batch : 110 , avg_train_loss =  0.040110560 ( 99.200% ) , avg_valid_loss =  0.045764860 ( 98.600% )\n",
            "Epoch : 06 Batch : 055 , avg_train_loss =  0.019015806 ( 99.000% ) , avg_valid_loss =  0.021947643 ( 99.000% )\n",
            "Epoch : 06 Batch : 110 , avg_train_loss =  0.034328174 ( 99.200% ) , avg_valid_loss =  0.042236759 ( 98.400% )\n",
            "Epoch : 07 Batch : 055 , avg_train_loss =  0.015110036 ( 98.600% ) , avg_valid_loss =  0.020507626 ( 98.200% )\n",
            "Epoch : 07 Batch : 110 , avg_train_loss =  0.029469925 ( 99.400% ) , avg_valid_loss =  0.039040096 ( 99.200% )\n",
            "Epoch : 08 Batch : 055 , avg_train_loss =  0.011155978 ( 99.600% ) , avg_valid_loss =  0.018495205 ( 99.000% )\n",
            "Epoch : 08 Batch : 110 , avg_train_loss =  0.023691568 ( 99.200% ) , avg_valid_loss =  0.037098214 ( 97.800% )\n",
            "Epoch : 09 Batch : 055 , avg_train_loss =  0.009623234 ( 99.600% ) , avg_valid_loss =  0.017724835 ( 99.400% )\n",
            "Epoch : 09 Batch : 110 , avg_train_loss =  0.020607111 ( 99.600% ) , avg_valid_loss =  0.035971635 ( 99.000% )\n",
            "Epoch : 10 Batch : 055 , avg_train_loss =  0.008049866 ( 99.200% ) , avg_valid_loss =  0.017657544 ( 98.800% )\n",
            "Epoch : 10 Batch : 110 , avg_train_loss =  0.017673195 ( 99.600% ) , avg_valid_loss =  0.034463916 ( 99.200% )\n",
            "학습 종료\n",
            "Accuracy : 99.200%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}